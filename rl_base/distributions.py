import torch
from torch.distributions import Categorical
from torch.distributions import Normal
from abc import ABC,abstractmethod

kl_div = torch.distributions.kl_divergence
class Distribution(ABC):
    def __init__(self):
        super(Distribution,self).__init__()
        self.distribution = None
    @abstractmethod
    def set_param(self,*args):
        raise NotImplementedError
    @abstractmethod
    def get_param(self):
        raise NotImplementedError
    @abstractmethod
    def log_prob(self,x:torch.Tensor):
        raise NotImplementedError
    @abstractmethod
    def entropy(self):
        raise NotImplementedError
    @abstractmethod
    def stochastic_sample(self):
        raise NotImplementedError
    @abstractmethod
    def deterministic_sample(self):
        raise NotImplementedError

class CategoricalDistribution(Distribution):
    def __init__(self,action_dim:int):
        super(CategoricalDistribution,self).__init__()
        self.action_dim = action_dim
    def set_param(self,logits):
        self.logits = logits
        self.distribution = Categorical(logits=logits)
    def get_param(self):
        return self.logits
    def log_prob(self,x):
        return self.distribution.log_prob(x)
    def entropy(self):
        return self.distribution.entropy()
    def stochastic_sample(self):
        return self.distribution.sample()
    def deterministic_sample(self):
        return torch.argmax(self.distribution.probs,dim=1)
    def kl_divergence(self,other:Distribution):
        assert isinstance(other,CategoricalDistribution), "KL Divergence should be measured by two same distribution with the same type"
        return kl_div(self.distribution,other.distribution)
    
class DiagGaussianDistribution(Distribution):
    def __init__(self,action_dim:int):
        super(DiagGaussianDistribution,self).__init__()
        self.action_dim = action_dim
    def set_param(self,mu,std):
        self.mu = mu
        self.std = std
        self.distribution = Normal(mu,std)
    def get_param(self):
        return self.mu,self.std
    def log_prob(self,x):
        return self.distribution.log_prob(x).sum(-1)
    def entropy(self):
        return self.distribution.entropy().sum(-1)
    def stochastic_sample(self):
        return self.distribution.sample()
    def deterministic_sample(self):
        return self.mu
    def kl_divergence(self,other:Distribution):
        assert isinstance(other,DiagGaussianDistribution), "KL Divergence should be measured by two same distribution with the same type"
        return kl_div(self.distribution,other.distribution)
    